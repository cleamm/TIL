{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jalammar.github.io/illustrated-gpt2/\n",
    "# GPT 작동 원리\n",
    "# GPT2는 한번에 하나의 token을 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20(Cls%2C%20Chatbot%2C%20NLI)/22-2.%20kogpt2_text_generation_gpu.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoGPT2로 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '근육이 커지기 위해서는'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[33245 10114 12748 11357]], shape=(1, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33245, 10114, 12748, 11357, 23879, 39306, 9684, 7884, 10211, 15177, 26421, 387, 17339, 7889, 9908, 15768, 6903, 15386, 8146, 12923, 9228, 18651, 42600, 9564, 17764, 9033, 9199, 14441, 7335, 8704, 12557, 32030, 9510, 18595, 9025, 10571, 25741, 10599, 13229, 9508, 7965, 8425, 33102, 9122, 21240, 9801, 32106, 13579, 12442, 13235, 19430, 8022, 12972, 9566, 11178, 9554, 24873, 7198, 9391, 12486, 8711, 9346, 7071, 36736, 9693, 12006, 9038, 10279, 36122, 9960, 8405, 10826, 18988, 25998, 9292, 7671, 9465, 7489, 9277, 10137, 9677, 9248, 9912, 12834, 11488, 13417, 7407, 8428, 8137, 9430, 14222, 11356, 10061, 9885, 19265, 9377, 20305, 7991, 9178, 9648, 9133, 10021, 10138, 30315, 21833, 9362, 9301, 9685, 11584, 9447, 42129, 10124, 7532, 17932, 47123, 37544, 9355, 15632, 9124, 10536, 13530, 12204, 9184, 36152, 9673, 9788, 9029, 11764]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids,\n",
    "                        max_length=128,\n",
    "                        repetition_penalty=2.0,\n",
    "                        use_cache=True)\n",
    "output_ids = output.numpy().tolist()[0]\n",
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\\n특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\\n또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\\n아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\\n운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\\n운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\\n운동을'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOP 5 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 51200), dtype=float32, numpy=\n",
       "array([[[-5.2999744 , -5.247863  , -4.796324  , ...,  0.02473605,\n",
       "         -1.145249  , -0.6509337 ],\n",
       "        [-4.790443  , -5.4123983 , -4.876633  , ..., -0.9312227 ,\n",
       "         -3.6518145 , -5.819604  ],\n",
       "        [-4.0496645 , -3.5694022 , -5.737194  , ..., -2.4115632 ,\n",
       "         -2.4622674 , -3.5288236 ],\n",
       "        [-5.456258  , -6.02514   , -6.1407743 , ..., -0.22475755,\n",
       "         -4.0253325 , -3.2478456 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 51200])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = tf.math.top_k(output.logits[0, -1], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁무엇보다', '▁우선', '▁반드시', '▁피부', '▁무엇보다도']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(top5.indices.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "while len(input_ids) < 50:\n",
    "    output = model(np.array([input_ids]))\n",
    "    top5 = tf.math.top_k(output.logits[0, -1], k=5)\n",
    "    token_id = random.choice(top5.indices.numpy())\n",
    "    input_ids.append(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'근육이 커지기 위해서는 무엇보다도 균형감각을 유지하는데 힘써주어야 할 것이다.\\n특히 요즘같이 날씨로 쌀뜨물 같은 날씨에도 쉽게 잠이 드는 계절이 되면 잠들기는 쉽지 않을 것이다.</d> 이 같은 현상은 올해도 지속돼 올해는 전 세계'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20(Cls%2C%20Chatbot%2C%20NLI)/22-3.%20kogpt2_chatbot_gpu.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='', eos_token='', pad_token='')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51200\n",
      "51200\n",
      "51200\n",
      "----------\n",
      "</s>\n",
      "<usr>\n",
      "<pad>\n",
      "<sys>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print('-' * 10)\n",
    "print(tokenizer.decode(1))\n",
    "print(tokenizer.decode(2))\n",
    "print(tokenizer.decode(3))\n",
    "print(tokenizer.decode(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_data():\n",
    "  for question, answer in zip(train_data.Q.to_list(), train_data.A.to_list()):\n",
    "    bos_token = [tokenizer.bos_token_id]\n",
    "    eos_token = [tokenizer.eos_token_id]\n",
    "    sent = tokenizer.encode('' + question + '' + answer) \n",
    "    yield bos_token + sent + eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[51200  9349  7888   739  7318   376 25000  6824  9108  9028  7098 25856\n",
      "  51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9020  8263  7497 10192 11615  8210  8006 11567  8711  9535  7483\n",
      "  12521 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9085  7597   395  8149 10624  7397 24224 13358  7182  8030 19138\n",
      "  16899  9677  8234   389 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9085  7597   395  8149  9465 10624  7397 24224 13358  7182  8030\n",
      "  19138 16899  9677  8234   389 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9943   422   418  9327  8702  7098  7141 16912 18328  8671  7415\n",
      "   8263  8234   389 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9815   410 21249 10174  6824  8210  8006 16146 11056 11594 10137\n",
      "  10556  9266  8711 25856 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9815   410 21249  9183  7249 16146 11056 11594 10137 10556  9266\n",
      "   8711 25856 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9815 37655  9622  8619 10401  9183  9328   216  9443 29490  9846\n",
      "   9788  9341 25856 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9815 37655 10135  7066 39488  9122  9050  9668 16576  9277  9044\n",
      "  26519 19658  9098  7652  7801 25856 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9815 37655 10135  7066  7692 11848  9042  7019 20284  7254 26519\n",
      "  19658  9098  7652  7801 25856 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9815 37655 18381  9063  7489 29615  9054 15730 29452 13474  7380\n",
      "   9033 10300 23775 25856 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 19319 48397  8711  6947 19858 27031  9122  8046 25856 51200 51200\n",
      "  51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 19319 46651 27481 48397  8711  6947 19858 27031  9122  8046 25856\n",
      "  51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 19319  8135  9749 10225  6866  9677  7182  8749  9589 20540  7801\n",
      "  25856 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 17230 17429  9160  8098  7237  8135  9427 35813  9122  8046 25856\n",
      "  51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 47980 22227 26992  7058  7182  7307  8137  9376  8737  8236  7801\n",
      "  25856 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 26629 23799   739  8308  7304 10174  8707 10247 16346  6889  9282\n",
      "   8400  7601  9078  7801 25856 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 15983  7673 24648  6889 25880  8006 27659 15582 46439 35557  6889\n",
      "  12252  7801 25856 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 15983  7673 24648 15010 10926  6853 27511 27659 15582 46439 35557\n",
      "   6889 12252  7801 25856 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 15983  7692 12371  9564 16409  9016  7182  8139  9271  9052  9267\n",
      "  27545  8711  7661 25856 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 15983  7692 36684  7220  9244  6958  9539  7478  6872  8006  7303\n",
      "   7359  9124  9024  7801  8084   376 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 15983  7692 26873  9050  7177  7182  8139  9271  9052  9267 27545\n",
      "   8711  7661 25856 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200  9278 20861  9193   739  7570 47804 22864 20861 32392 10070 10828\n",
      "  25856  9105 12114  9094 12191 12700 31279  8702 38887 15148 35441  9328\n",
      "   9109  7801 25856 51200]\n",
      " [51200 10464 12079  9028  9926  9651  8006  8054 27820  9432 23100 21833\n",
      "  14247 29462  7801 25856 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464 12079 17577  8054 27820  9432 23100 21833 14247 29462  7801\n",
      "  25856 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464 12079 42076  9340   406  8054 27820  9432 23100 21833 14247\n",
      "  29462  7801 25856 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464  9341   406 35628  9659  9701 11389 11676  7177   387  9265\n",
      "   7380 11120  8711 10764 11389  9728 12245 22238  9341  8084 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464 10143  9666   739  8244 35628  9659  9701 11389 11676  7177\n",
      "    387  9265  7380 11120  8711 10764 11389  9728 12245 22238  9341  8084\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464 18264 12079  6826  9016  7208 25772  8267 25012  9069  6872\n",
      "   7098 25856 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464  7285 10056 25799  8185  7235 25856 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464  9136  7380  9071  7513  8711  7182  7285  9117  7703  7788\n",
      "  11120  8705 14553 10667  8718  7055  7661 25856 51200 51200 51200 51200\n",
      "  51200 51200 51200 51200]\n",
      " [51200 10464  9136  7380  9071  7513  8711  8210  8006  7182  7285  9117\n",
      "   7703  7788 11120  8705 14553 10667  8718  7055  7661 25856 51200 51200\n",
      "  51200 51200 51200 51200]], shape=(32, 28), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> 12시 땡!하루가 또 가네요.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[51200  9349  7888   739  7318   376 25000  6824  9108  9028  7098 25856\n",
      " 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200 51200\n",
      " 51200 51200 51200 51200], shape=(28,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9349, 7888, 739, 7318, 376, 12557, 6824, 9108, 9028, 7098, 25856]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(' 12시 땡! 하루가 또 가네요.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    }
   ],
   "source": [
    "steps = len(train_data) // batch_size + 1\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch in tqdm.tqdm_notebook(dataset, total=steps):\n",
    "      with tf.GradientTape() as tape:\n",
    "          result = model(batch, labels=batch)\n",
    "          loss = result[0]\n",
    "          batch_loss = tf.reduce_mean(loss)\n",
    "          \n",
    "      grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "      adam.apply_gradients(zip(grads, model.trainable_variables))\n",
    "      epoch_loss += batch_loss / steps\n",
    "\n",
    "  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '오늘도 좋은 하루!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '' + text + ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=50, early_stopping=True, eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentence = tokenizer.decode(output[0].numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentence.split(' ')[1].replace('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=50, do_sample=True, top_k=10)\n",
    "tokenizer.decode(output[0].numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_answer_by_chatbot(user_text):\n",
    "  sent = '' + user_text + ''\n",
    "  input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n",
    "  input_ids = tf.convert_to_tensor([input_ids])\n",
    "  output = model.generate(input_ids, max_length=50, do_sample=True, top_k=20)\n",
    "  sentence = tokenizer.decode(output[0].numpy().tolist())\n",
    "  chatbot_response = sentence.split(' ')[1].replace('', '')\n",
    "  return chatbot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_answer_by_chatbot('안녕! 반가워~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형식에 맞는 텍스트 데이터 준비, 모델 생성, 모델 활용\n",
    "# https://platform.openai.com/docs/guides/fine-tuning 파인튜닝 가이드"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
